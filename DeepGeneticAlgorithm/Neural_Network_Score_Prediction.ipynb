{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarchMayhem/4644_final/blob/main/Neural_Network_Score_Prediction.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o7qqP1qs5Rp6",
        "outputId": "846cf9e4-8f93-4e5e-d22d-36dcec19b4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting C server in the background...\n",
            ".....................\n",
            "Success: Server is responsive at http://localhost:8888/.\n",
            "Server is running with PID: 13414\n"
          ]
        }
      ],
      "source": [
        "# @title Setup the API Server\n",
        "\n",
        "# Installing Dependencies\n",
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y build-essential libmicrohttpd-dev libjson-c-dev xz-utils\n",
        "!pip install requests -q\n",
        "\n",
        "# Installing API Server\n",
        "!wget https://github.com/RusDoomer/SVOBODA/archive/refs/tags/v0.1.tar.gz -O svoboda.tar.gz -q\n",
        "!tar -xzf svoboda.tar.gz\n",
        "!cd SVOBODA-0.1 && make\n",
        "!wget https://colemak.com/pub/corpus/iweb-corpus-samples-cleaned.txt.xz -q\n",
        "!unxz iweb-corpus-samples-cleaned.txt.xz\n",
        "!mv iweb-corpus-samples-cleaned.txt SVOBODA-0.1/data/english/corpora/shai.txt\n",
        "\n",
        "# Starting the API Server\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import signal\n",
        "\n",
        "# --- Configuration ---\n",
        "server_directory = 'SVOBODA-0.1'\n",
        "executable_name = './svoboda'\n",
        "api_url = \"http://localhost:8888/\"\n",
        "startup_timeout_seconds = 60\n",
        "\n",
        "# --- Process Handling ---\n",
        "server_process = None\n",
        "\n",
        "if not os.path.exists(os.path.join(server_directory, executable_name)):\n",
        "    print(\"Error: Server executable not found. Please run the setup cell first.\")\n",
        "else:\n",
        "    print(\"Starting C server in the background...\")\n",
        "    # Launch the server in the background\n",
        "    server_process = subprocess.Popen(\n",
        "        [executable_name],\n",
        "        cwd=server_directory,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # --- Verification Loop ---\n",
        "    server_ready = False\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < startup_timeout_seconds:\n",
        "        # Check if the process has already terminated\n",
        "        if server_process.poll() is not None:\n",
        "            print(f\"Error: Server process terminated unexpectedly with exit code {server_process.poll()}.\")\n",
        "            # Print stderr for debugging\n",
        "            for line in server_process.stderr.readlines():\n",
        "                print(f\"[SERVER_STDERR] {line.strip()}\")\n",
        "            break\n",
        "\n",
        "        # Attempt to connect\n",
        "        try:\n",
        "            response = requests.get(api_url, timeout=1)\n",
        "            print(f\"\\nSuccess: Server is responsive at {api_url}.\")\n",
        "            server_ready = True\n",
        "            break\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    if not server_ready and server_process.poll() is None:\n",
        "        print(\"\\nError: Server did not become responsive within the timeout period.\")\n",
        "        print(\"The process is still running but may be stuck. Terminating.\")\n",
        "        server_process.terminate()\n",
        "    elif server_ready:\n",
        "        print(f\"Server is running with PID: {server_process.pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IFZylxgJjy_z"
      },
      "outputs": [],
      "source": [
        "# Required Imports for the Neural Network\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, optimizers\n",
        "\n",
        "# The symbols whose keyboard positions we are optimizing\n",
        "# Layouts will be represented as lists of indices from this symbol set\n",
        "symbols = \"abcdefghijklmnopqrstuvwxyz;,./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dU-ajMq0N7S",
        "outputId": "0ef4a240-0aa7-4b44-8620-b89d2dd11561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You can attach your Google Drive and create the same training data files as we did, but it isn't required\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lpdXhaWZ0bTc"
      },
      "outputs": [],
      "source": [
        "# The Basic Unit of our Neural Network. This defines a custom Fully Connected Layer with a Leaky ReLU activation function and skip connection around the entire layer.\n",
        "class CustomLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom Keras layer that combines a Dense layer, Leaky ReLU activation,\n",
        "    and a conditional skip connection.\n",
        "\n",
        "    The skip connection adds the original input to the output of the\n",
        "    Dense -> Leaky ReLU sequence ONLY if the last dimension of the input\n",
        "    matches the number of units in the Dense layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, negative_slope=0.5, l2_reg=0.0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the custom layer.\n",
        "\n",
        "        Args:\n",
        "            units (int): The number of output units for the Dense layer.\n",
        "            alpha (float): The negative slope coefficient for the Leaky ReLU activation.\n",
        "            **kwargs: Additional keyword arguments to pass to the base Layer class.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.negative_slope = negative_slope\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        kernel_regularizer = regularizers.l2(self.l2_reg)\n",
        "\n",
        "\n",
        "        # Initialize the Dense layer and Leaky ReLU activation\n",
        "        # These will create their weights when first called (or in build if explicitly built)\n",
        "        self.dense_layer = layers.Dense(self.units, kernel_regularizer=kernel_regularizer, name=\"dense_part\")\n",
        "        self.leaky_relu_activation = layers.LeakyReLU(negative_slope=self.negative_slope, name=\"leaky_relu_part\")\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Builds the layer's weights. This method is called automatically\n",
        "        the first time the layer is run.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tf.TensorShape): The shape of the input tensor.\n",
        "                                          Typically (batch_size, ..., input_dim).\n",
        "        \"\"\"\n",
        "        # The dense layer's weights are implicitly built when it's called\n",
        "        # with an input of known shape. We explicitly build it here to\n",
        "        # ensure its weights are created before the first call, which can\n",
        "        # be useful for inspection or if the layer is part of a larger model\n",
        "        # that needs to know all its weights upfront.\n",
        "        self.dense_layer.build(input_shape)\n",
        "\n",
        "        # Determine if a skip connection is possible based on input and output dimensions.\n",
        "        # The skip connection adds the original input to the processed output.\n",
        "        # For this to work, the last dimension of the input must match the number of units.\n",
        "        self.can_skip_connect = (input_shape[-1] == self.units)\n",
        "        if not self.can_skip_connect:\n",
        "            print(f\"Warning: Skip connection not possible for layer '{self.name}'.\")\n",
        "            print(f\"Input last dimension ({input_shape[-1]}) does not match Dense units ({self.units}).\")\n",
        "            print(\"The layer will function as a standard Dense + Leaky ReLU.\")\n",
        "\n",
        "        super().build(input_shape) # Call the base class's build method\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass logic of the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input tensor to the layer.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output tensor after applying Dense, Leaky ReLU,\n",
        "                       and the conditional skip connection.\n",
        "        \"\"\"\n",
        "        # Step 1: Pass through the Dense layer\n",
        "        dense_output = self.dense_layer(inputs)\n",
        "\n",
        "        # Step 2: Apply Leaky ReLU activation\n",
        "        activated_output = self.leaky_relu_activation(dense_output)\n",
        "\n",
        "        # Step 3: Implement the conditional skip connection\n",
        "        # We check the actual shape of the `inputs` tensor at call time.\n",
        "        # Note: input_shape from build might have None for batch size,\n",
        "        # but inputs.shape will have the concrete batch size.\n",
        "        # We only care about the feature dimension (last dimension).\n",
        "        if inputs.shape[-1] == self.units:\n",
        "            # If dimensions match, add the original input to the activated output\n",
        "            output = activated_output + inputs\n",
        "            # print(f\"Info: Skip connection applied for layer '{self.name}'.\")\n",
        "        else:\n",
        "            # If dimensions don't match, no skip connection is applied\n",
        "            output = activated_output\n",
        "            # The warning about skip connection not being possible is already printed in build,\n",
        "            # but we can add a runtime message here if needed for debugging.\n",
        "            # print(f\"Info: Skip connection NOT applied for layer '{self.name}' due to dimension mismatch.\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the serializable configuration of the layer.\n",
        "        This is important for saving and loading models that use custom layers.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"units\": self.units,\n",
        "            \"negative_slope\": self.negative_slope,\n",
        "            \"l2_reg\": self.l2_reg\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5x5yAkTLj6SQ"
      },
      "outputs": [],
      "source": [
        "# Calls the API Server to evaluate a batch of layouts which utilize the provided string of symbols\n",
        "# If metric weights is set to None, then this function will return each of the five individual metrics for each of the input layouts\n",
        "# Otherwise, it will return the weighted sum of these metrics using the provided metric weights\n",
        "def true_evaluation(batch, metric_weights, symbols):\n",
        "  layouts = []\n",
        "  for a in batch:\n",
        "    layouts.append(\"\")\n",
        "    for index in a:\n",
        "      layouts[-1] += symbols[index]\n",
        "\n",
        "  api_url = \"http://localhost:8888/\"\n",
        "\n",
        "  payload = [{\n",
        "    \"layout\": layout,\n",
        "    \"weights\":  {\"sfb\": 0.0, \"sfs\": 0.0, \"lsb\": 0.0, \"alt\": 0.0, \"rolls\": 0.0} if metric_weights == None else metric_weights\n",
        "  } for layout in layouts]\n",
        "\n",
        "  try:\n",
        "    response = requests.post(api_url, json=payload)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "      print(f\"Error: Server responded with status code {response.status_code}\")\n",
        "      print(\"Response text:\", response.text)\n",
        "    else:\n",
        "      if metric_weights == None:\n",
        "        return [[r[\"stat_values\"][\"sfb\"], r[\"stat_values\"][\"sfs\"], r[\"stat_values\"][\"lsb\"], r[\"stat_values\"][\"alt\"], r[\"stat_values\"][\"rolls\"]] for r in response.json()]\n",
        "      else:\n",
        "        return [r[\"score\"] for r in response.json()]\n",
        "\n",
        "  except requests.exceptions.ConnectionError as e:\n",
        "    print(\"Connection Error: Could not connect to the C server.\")\n",
        "    print(\"Please ensure the server is running correctly from the previous cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-IGGpFRTL4e5"
      },
      "outputs": [],
      "source": [
        "# Creates our Neural Network and its Optimizer\n",
        "# For the sake of efficiency, the network assumes you have already created the one-hot encodings of the layouts\n",
        "# The random seed will often be set at the top of each cell to ensure you can reproduce our results\n",
        "# These seed values were themselves chosen randomly with no cherry picking\n",
        "np.random.seed(346)\n",
        "\n",
        "loss_fn = tf.keras.losses.MSE\n",
        "architecture = [\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  layers.Dense(5)\n",
        "]\n",
        "\n",
        "model = models.Sequential(architecture)\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "model.build(input_shape = (None, 900))\n",
        "trainable_variables = model.trainable_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gVI1VAWh9dtD",
        "outputId": "95d2eded-243b-48aa-d948-3d5f6fda5aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "12\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "13\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "14\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "15\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "16\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "17\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "18\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "19\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "20\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n"
          ]
        }
      ],
      "source": [
        "# Running this will require having your Google Drive attached!\n",
        "# This is what we used to create the training data for the model\n",
        "# The training data is also already provided to in the github though\n",
        "for t in range(1, 11):\n",
        "  print(t)\n",
        "  random.seed([605, 505, 108, 259, 341, 215, 971, 297, 735, 973][t - 1])\n",
        "\n",
        "  # These are the means (offsets) and mean absolute deviations (scalars) of each of the five metrics\n",
        "  # These were calculated from a set of 10,000 uniformly random layouts\n",
        "  scalars = np.array([2.37272293, 1.77984022, 1.84450506, 2.34851446, 2.97247169])\n",
        "  offsets = np.array([10.53548093, 10.47626445,  3.99833419, 17.61765303, 35.32753762])\n",
        "\n",
        "  # Generates uniformly random layouts and calculates their metric scores which are normalized with the scalars and offsets lists above\n",
        "  layouts = []\n",
        "  scores = []\n",
        "  a = [i for i in range(30)]\n",
        "  for i in range(2000):\n",
        "    if i % 100 == 0:\n",
        "      print(i)\n",
        "    batch = []\n",
        "    for j in range(500):\n",
        "      random.shuffle(a)\n",
        "      batch.append(copy.deepcopy(a))\n",
        "      layouts.append(copy.deepcopy(a))\n",
        "    scores += list((np.array(true_evaluation(batch, None, symbols)) - offsets)/scalars)\n",
        "\n",
        "  L1 = layers.CategoryEncoding(num_tokens = 30, output_mode = \"one_hot\")\n",
        "  L2 = layers.Flatten()\n",
        "  inputs = L2(L1(np.array(layouts)))\n",
        "\n",
        "  # Saves the resulting data as pickle files in your google drive\n",
        "  save_path = '/content/drive/My Drive/colab_data/'\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  with open(os.path.join(save_path, 'layouts_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(layouts, f)\n",
        "  with open(os.path.join(save_path, 'inputs_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(inputs, f)\n",
        "  with open(os.path.join(save_path, 'scores_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(np.array(scores), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x4C4tOIHB7OQ"
      },
      "outputs": [],
      "source": [
        "# If you don't wish to train the neural network, this will load\n",
        "save_path = '/content/drive/My Drive/colab_data/'\n",
        "with open(os.path.join(save_path, 'eight_hidden_layer_network_weights.pkl'), 'rb') as f:\n",
        "    model.set_weights(pickle.load(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeVaSnhOZiZ8",
        "outputId": "6dcd5d87-20d9-41f0-c400-a6a63b5d04e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9906e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9650e-04 - val_loss: 0.0014\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9887e-04 - val_loss: 0.0013\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9284e-04 - val_loss: 0.0013\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.8811e-04 - val_loss: 0.0014\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.8033e-04 - val_loss: 0.0013\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9799e-04 - val_loss: 0.0013\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9977e-04 - val_loss: 0.0013\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9476e-04 - val_loss: 0.0013\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7846e-04 - val_loss: 0.0016\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.8273e-04 - val_loss: 0.0013\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.7067e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.6475e-04 - val_loss: 0.0013\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9735e-04 - val_loss: 0.0013\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.8721e-04 - val_loss: 0.0013\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9196e-04 - val_loss: 0.0012\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8366e-04 - val_loss: 0.0013\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7067e-04 - val_loss: 0.0013\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8042e-04 - val_loss: 0.0013\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7841e-04 - val_loss: 0.0013\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6676e-04 - val_loss: 0.0012\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5070e-04 - val_loss: 0.0013\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4739e-04 - val_loss: 0.0013\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9164e-04 - val_loss: 0.0013\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9305e-04 - val_loss: 0.0014\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8232e-04 - val_loss: 0.0013\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6819e-04 - val_loss: 0.0013\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6195e-04 - val_loss: 0.0012\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5358e-04 - val_loss: 0.0013\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5357e-04 - val_loss: 0.0011\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.3777e-04 - val_loss: 0.0013\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4372e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4002e-04 - val_loss: 0.0014\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9843e-04 - val_loss: 0.0012\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8287e-04 - val_loss: 0.0012\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8001e-04 - val_loss: 0.0015\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7623e-04 - val_loss: 0.0012\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.6370e-04 - val_loss: 0.0012\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.5867e-04 - val_loss: 0.0012\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.4253e-04 - val_loss: 0.0012\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.4167e-04 - val_loss: 0.0012\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.3509e-04 - val_loss: 0.0013\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.3569e-04 - val_loss: 0.0012\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.2605e-04 - val_loss: 0.0013\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.3177e-04 - val_loss: 0.0015\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9411e-04 - val_loss: 0.0012\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.8429e-04 - val_loss: 0.0012\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6735e-04 - val_loss: 0.0012\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6120e-04 - val_loss: 0.0012\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.6413e-04 - val_loss: 0.0011\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4847e-04 - val_loss: 0.0012\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5188e-04 - val_loss: 0.0014\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.3758e-04 - val_loss: 0.0012\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.3491e-04 - val_loss: 0.0011\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2230e-04 - val_loss: 0.0012\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2439e-04 - val_loss: 0.0011\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.0396e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.1219e-04 - val_loss: 0.0013\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.8626e-04 - val_loss: 0.0012\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.7960e-04 - val_loss: 0.0013\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.6900e-04 - val_loss: 0.0012\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.5257e-04 - val_loss: 0.0012\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.5669e-04 - val_loss: 0.0011\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.4101e-04 - val_loss: 0.0012\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2851e-04 - val_loss: 0.0011\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2722e-04 - val_loss: 0.0012\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2021e-04 - val_loss: 0.0013\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.1961e-04 - val_loss: 0.0012\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.1430e-04 - val_loss: 0.0013\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.0405e-04 - val_loss: 0.0012\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.0651e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 8.9671e-04 - val_loss: 0.0012\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.9608e-04 - val_loss: 0.0013\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7493e-04 - val_loss: 0.0012\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.7123e-04 - val_loss: 0.0012\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5040e-04 - val_loss: 0.0012\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4331e-04 - val_loss: 0.0011\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.4597e-04 - val_loss: 0.0012\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.3404e-04 - val_loss: 0.0011\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2040e-04 - val_loss: 0.0013\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.2642e-04 - val_loss: 0.0012\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.1101e-04 - val_loss: 0.0011\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.0836e-04 - val_loss: 0.0012\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.0066e-04 - val_loss: 0.0012\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 8.9426e-04 - val_loss: 0.0011\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 8.8956e-04 - val_loss: 0.0012\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 8.8765e-04 - val_loss: 0.0012\n",
            "Epoch 1/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 2/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 3/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 4/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 5/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.9695e-04 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.7482e-04 - val_loss: 0.0011\n",
            "Epoch 7/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.6208e-04 - val_loss: 0.0013\n",
            "Epoch 8/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.5424e-04 - val_loss: 0.0012\n",
            "Epoch 9/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.5213e-04 - val_loss: 0.0012\n",
            "Epoch 10/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.3817e-04 - val_loss: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.2992e-04 - val_loss: 0.0011\n",
            "Epoch 12/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.1550e-04 - val_loss: 0.0012\n",
            "Epoch 13/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 9.1497e-04 - val_loss: 0.0011\n",
            "Epoch 14/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 9.1020e-04 - val_loss: 0.0011\n",
            "Epoch 15/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 8.9759e-04 - val_loss: 0.0011\n",
            "Epoch 16/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 8.9070e-04 - val_loss: 0.0012\n",
            "Epoch 17/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 8.9211e-04 - val_loss: 0.0011\n",
            "Epoch 18/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 8.8889e-04 - val_loss: 0.0012\n",
            "Epoch 19/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 8.7730e-04 - val_loss: 0.0011\n",
            "Epoch 20/20\n",
            "\u001b[1m3868/3868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 8.7688e-04 - val_loss: 0.0011\n"
          ]
        }
      ],
      "source": [
        "# np.random.seed(152)\n",
        "# np.random.seed(370)\n",
        "# np.random.seed(878)\n",
        "# np.random.seed(265)\n",
        "\n",
        "np.random.seed(690)\n",
        "for i in range(11, 21):\n",
        "  save_path = '/content/drive/My Drive/colab_data/'\n",
        "  with open(os.path.join(save_path, 'inputs_large_' + str(i) + '.pkl'), 'rb') as f:\n",
        "    inputs = pickle.load(f)\n",
        "  with open(os.path.join(save_path, 'scores_large_' + str(i) + '.pkl'), 'rb') as f:\n",
        "    scores = pickle.load(f)\n",
        "\n",
        "  model.fit(epochs = 20, shuffle = True, x = inputs, y = np.array(scores), batch_size = 256, validation_split = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62hLK0YH3Hcx",
        "outputId": "1607d823-b802-4ed5-c6a6-209dda88ce27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float32(0.0014341974)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.seed(203)\n",
        "\n",
        "layouts = [[i for i in range(30)] for j in range(100000)]\n",
        "for a in layouts:\n",
        "  random.shuffle(a)\n",
        "\n",
        "np.mean(loss_fn((np.array(true_evaluation(layouts, None, symbols) - offsets)/scalars), model(L2(L1(np.array(layouts))))))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
