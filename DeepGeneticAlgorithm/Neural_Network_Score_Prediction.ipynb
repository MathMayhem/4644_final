{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFqyyafOnsk"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathMayhem/4644_final/blob/main/DeepGeneticAlgorithm/Neural_Network_Score_Prediction.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7qqP1qs5Rp6"
      },
      "outputs": [],
      "source": [
        "# Setup the API Server\n",
        "\n",
        "# Installing Dependencies\n",
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y build-essential libmicrohttpd-dev libjson-c-dev xz-utils\n",
        "!pip install requests -q\n",
        "\n",
        "# Installing API Server\n",
        "!wget https://github.com/RusDoomer/SVOBODA/archive/refs/tags/v0.1.tar.gz -O svoboda.tar.gz -q\n",
        "!tar -xzf svoboda.tar.gz\n",
        "!cd SVOBODA-0.1 && make\n",
        "!wget https://colemak.com/pub/corpus/iweb-corpus-samples-cleaned.txt.xz -q\n",
        "!unxz iweb-corpus-samples-cleaned.txt.xz\n",
        "!mv iweb-corpus-samples-cleaned.txt SVOBODA-0.1/data/english/corpora/shai.txt\n",
        "\n",
        "# Starting the API Server\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import signal\n",
        "\n",
        "# --- Configuration ---\n",
        "server_directory = 'SVOBODA-0.1'\n",
        "executable_name = './svoboda'\n",
        "api_url = \"http://localhost:8888/\"\n",
        "startup_timeout_seconds = 60\n",
        "\n",
        "# --- Process Handling ---\n",
        "server_process = None\n",
        "\n",
        "if not os.path.exists(os.path.join(server_directory, executable_name)):\n",
        "    print(\"Error: Server executable not found. Please run the setup cell first.\")\n",
        "else:\n",
        "    print(\"Starting C server in the background...\")\n",
        "    # Launch the server in the background\n",
        "    server_process = subprocess.Popen(\n",
        "        [executable_name],\n",
        "        cwd=server_directory,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # --- Verification Loop ---\n",
        "    server_ready = False\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < startup_timeout_seconds:\n",
        "        # Check if the process has already terminated\n",
        "        if server_process.poll() is not None:\n",
        "            print(f\"Error: Server process terminated unexpectedly with exit code {server_process.poll()}.\")\n",
        "            # Print stderr for debugging\n",
        "            for line in server_process.stderr.readlines():\n",
        "                print(f\"[SERVER_STDERR] {line.strip()}\")\n",
        "            break\n",
        "\n",
        "        # Attempt to connect\n",
        "        try:\n",
        "            response = requests.get(api_url, timeout=1)\n",
        "            print(f\"\\nSuccess: Server is responsive at {api_url}.\")\n",
        "            server_ready = True\n",
        "            break\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    if not server_ready and server_process.poll() is None:\n",
        "        print(\"\\nError: Server did not become responsive within the timeout period.\")\n",
        "        print(\"The process is still running but may be stuck. Terminating.\")\n",
        "        server_process.terminate()\n",
        "    elif server_ready:\n",
        "        print(f\"Server is running with PID: {server_process.pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFZylxgJjy_z"
      },
      "outputs": [],
      "source": [
        "# Required Imports for the Neural Network\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, optimizers\n",
        "\n",
        "# The symbols whose keyboard positions we are optimizing\n",
        "# Layouts will be represented as lists of indices from this symbol set\n",
        "symbols = \"abcdefghijklmnopqrstuvwxyz;,./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dU-ajMq0N7S"
      },
      "outputs": [],
      "source": [
        "# Mount your drive to use our data files or generate your own\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpdXhaWZ0bTc"
      },
      "outputs": [],
      "source": [
        "# The Basic Unit of our Neural Network. This defines a custom Fully Connected Layer with a Leaky ReLU activation function and skip connection around the entire layer.\n",
        "class CustomLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom Keras layer that combines a Dense layer, Leaky ReLU activation,\n",
        "    and a conditional skip connection.\n",
        "\n",
        "    The skip connection adds the original input to the output of the\n",
        "    Dense -> Leaky ReLU sequence ONLY if the last dimension of the input\n",
        "    matches the number of units in the Dense layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, negative_slope=0.5, l2_reg=0.0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the custom layer.\n",
        "\n",
        "        Args:\n",
        "            units (int): The number of output units for the Dense layer.\n",
        "            alpha (float): The negative slope coefficient for the Leaky ReLU activation.\n",
        "            **kwargs: Additional keyword arguments to pass to the base Layer class.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.negative_slope = negative_slope\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        kernel_regularizer = regularizers.l2(self.l2_reg)\n",
        "\n",
        "\n",
        "        # Initialize the Dense layer and Leaky ReLU activation\n",
        "        # These will create their weights when first called (or in build if explicitly built)\n",
        "        self.dense_layer = layers.Dense(self.units, kernel_regularizer=kernel_regularizer, name=\"dense_part\")\n",
        "        self.leaky_relu_activation = layers.LeakyReLU(negative_slope=self.negative_slope, name=\"leaky_relu_part\")\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Builds the layer's weights. This method is called automatically\n",
        "        the first time the layer is run.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tf.TensorShape): The shape of the input tensor.\n",
        "                                          Typically (batch_size, ..., input_dim).\n",
        "        \"\"\"\n",
        "        # The dense layer's weights are implicitly built when it's called\n",
        "        # with an input of known shape. We explicitly build it here to\n",
        "        # ensure its weights are created before the first call, which can\n",
        "        # be useful for inspection or if the layer is part of a larger model\n",
        "        # that needs to know all its weights upfront.\n",
        "        self.dense_layer.build(input_shape)\n",
        "\n",
        "        # Determine if a skip connection is possible based on input and output dimensions.\n",
        "        # The skip connection adds the original input to the processed output.\n",
        "        # For this to work, the last dimension of the input must match the number of units.\n",
        "        self.can_skip_connect = (input_shape[-1] == self.units)\n",
        "        if not self.can_skip_connect:\n",
        "            print(f\"Warning: Skip connection not possible for layer '{self.name}'.\")\n",
        "            print(f\"Input last dimension ({input_shape[-1]}) does not match Dense units ({self.units}).\")\n",
        "            print(\"The layer will function as a standard Dense + Leaky ReLU.\")\n",
        "\n",
        "        super().build(input_shape) # Call the base class's build method\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass logic of the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input tensor to the layer.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output tensor after applying Dense, Leaky ReLU,\n",
        "                       and the conditional skip connection.\n",
        "        \"\"\"\n",
        "        # Step 1: Pass through the Dense layer\n",
        "        dense_output = self.dense_layer(inputs)\n",
        "\n",
        "        # Step 2: Apply Leaky ReLU activation\n",
        "        activated_output = self.leaky_relu_activation(dense_output)\n",
        "\n",
        "        # Step 3: Implement the conditional skip connection\n",
        "        # We check the actual shape of the `inputs` tensor at call time.\n",
        "        # Note: input_shape from build might have None for batch size,\n",
        "        # but inputs.shape will have the concrete batch size.\n",
        "        # We only care about the feature dimension (last dimension).\n",
        "        if inputs.shape[-1] == self.units:\n",
        "            # If dimensions match, add the original input to the activated output\n",
        "            output = activated_output + inputs\n",
        "            # print(f\"Info: Skip connection applied for layer '{self.name}'.\")\n",
        "        else:\n",
        "            # If dimensions don't match, no skip connection is applied\n",
        "            output = activated_output\n",
        "            # The warning about skip connection not being possible is already printed in build,\n",
        "            # but we can add a runtime message here if needed for debugging.\n",
        "            # print(f\"Info: Skip connection NOT applied for layer '{self.name}' due to dimension mismatch.\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the serializable configuration of the layer.\n",
        "        This is important for saving and loading models that use custom layers.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"units\": self.units,\n",
        "            \"negative_slope\": self.negative_slope,\n",
        "            \"l2_reg\": self.l2_reg\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x5yAkTLj6SQ"
      },
      "outputs": [],
      "source": [
        "# Calls the API Server to evaluate a batch of layouts which utilize the provided string of symbols\n",
        "# If metric weights is set to None, then this function will return each of the five individual metrics for each of the input layouts\n",
        "# Otherwise, it will return the weighted sum of these metrics using the provided metric weights\n",
        "def true_evaluation(batch, metric_weights, symbols):\n",
        "  layouts = []\n",
        "  for a in batch:\n",
        "    layouts.append(\"\")\n",
        "    for index in a:\n",
        "      layouts[-1] += symbols[index]\n",
        "\n",
        "  api_url = \"http://localhost:8888/\"\n",
        "\n",
        "  payload = [{\n",
        "    \"layout\": layout,\n",
        "    \"weights\":  {\"sfb\": 0.0, \"sfs\": 0.0, \"lsb\": 0.0, \"alt\": 0.0, \"rolls\": 0.0} if metric_weights == None else metric_weights\n",
        "  } for layout in layouts]\n",
        "\n",
        "  try:\n",
        "    response = requests.post(api_url, json=payload)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "      print(f\"Error: Server responded with status code {response.status_code}\")\n",
        "      print(\"Response text:\", response.text)\n",
        "    else:\n",
        "      if metric_weights == None:\n",
        "        return [[r[\"stat_values\"][\"sfb\"], r[\"stat_values\"][\"sfs\"], r[\"stat_values\"][\"lsb\"], r[\"stat_values\"][\"alt\"], r[\"stat_values\"][\"rolls\"]] for r in response.json()]\n",
        "      else:\n",
        "        return [r[\"score\"] for r in response.json()]\n",
        "\n",
        "  except requests.exceptions.ConnectionError as e:\n",
        "    print(\"Connection Error: Could not connect to the C server.\")\n",
        "    print(\"Please ensure the server is running correctly from the previous cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IGGpFRTL4e5"
      },
      "outputs": [],
      "source": [
        "# Creates our Neural Network and its Optimizer\n",
        "# For the sake of efficiency, the network assumes you have already created the one-hot encodings of the layouts\n",
        "# The random seed will often be set at the top of each cell to ensure you can reproduce the results found in our paper\n",
        "# These seed values were themselves chosen randomly with no cherry picking\n",
        "np.random.seed(346)\n",
        "\n",
        "loss_fn = tf.keras.losses.MSE\n",
        "architecture = [\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  CustomLayer(900),\n",
        "  layers.Dense(5)\n",
        "]\n",
        "\n",
        "model = models.Sequential(architecture)\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "model.build(input_shape = (None, 900))\n",
        "trainable_variables = model.trainable_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gVI1VAWh9dtD"
      },
      "outputs": [],
      "source": [
        "# Running this will require having your Google Drive attached!\n",
        "# Requires significant compute!\n",
        "# This is what we used to create the training data for the model\n",
        "for t in range(1, 11):\n",
        "  print(t)\n",
        "  random.seed([605, 505, 108, 259, 341, 215, 971, 297, 735, 973][t - 1])\n",
        "\n",
        "  # These are the means (offsets) and mean absolute deviations (scalars) of each of the five metrics\n",
        "  # These were calculated from a set of 10,000 uniformly random layouts\n",
        "  scalars = np.array([2.37272293, 1.77984022, 1.84450506, 2.34851446, 2.97247169])\n",
        "  offsets = np.array([10.53548093, 10.47626445,  3.99833419, 17.61765303, 35.32753762])\n",
        "\n",
        "  # Generates uniformly random layouts and calculates their metric scores which are normalized with the scalars and offsets lists above\n",
        "  layouts = []\n",
        "  scores = []\n",
        "  a = [i for i in range(30)]\n",
        "  for i in range(2000):\n",
        "    if i % 100 == 0:\n",
        "      print(i)\n",
        "    batch = []\n",
        "    for j in range(500):\n",
        "      random.shuffle(a)\n",
        "      batch.append(copy.deepcopy(a))\n",
        "      layouts.append(copy.deepcopy(a))\n",
        "    scores += list((np.array(true_evaluation(batch, None, symbols)) - offsets)/scalars)\n",
        "\n",
        "  L1 = layers.CategoryEncoding(num_tokens = 30, output_mode = \"one_hot\")\n",
        "  L2 = layers.Flatten()\n",
        "  inputs = L2(L1(np.array(layouts)))\n",
        "\n",
        "  # Saves the resulting data as pickle files in your google drive\n",
        "  save_path = '/content/drive/My Drive/colab_data/'\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  with open(os.path.join(save_path, 'layouts_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(layouts, f)\n",
        "  with open(os.path.join(save_path, 'inputs_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(inputs, f)\n",
        "  with open(os.path.join(save_path, 'scores_large_' + str(t) + '.pkl'), 'wb') as f:\n",
        "      pickle.dump(np.array(scores), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4C4tOIHB7OQ"
      },
      "outputs": [],
      "source": [
        "# Running this will require having your Google Drive attached!\n",
        "# If you don't wish to train the neural network, this will load the network weights\n",
        "save_path = '/content/drive/My Drive/colab_data/'\n",
        "with open(os.path.join(save_path, 'eight_hidden_layer_network_weights.pkl'), 'rb') as f:\n",
        "    model.set_weights(pickle.load(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IeVaSnhOZiZ8"
      },
      "outputs": [],
      "source": [
        "# Running this will require having your Google Drive attached!\n",
        "# Requires significant compute! It is recommended that you use a GPU to speed up execution!\n",
        "# This is the code for training the network. Run this cell four times with each successive random seed value.\n",
        "# np.random.seed(152)\n",
        "# np.random.seed(370)\n",
        "# np.random.seed(878)\n",
        "# np.random.seed(265)\n",
        "\n",
        "for i in range(1, 11):\n",
        "  save_path = '/4644_final/DeepGeneticAlgorithm/Final Pickle Data/'\n",
        "  with open(os.path.join(save_path, 'inputs_large_' + str(i) + '.pkl'), 'rb') as f:\n",
        "    inputs = pickle.load(f)\n",
        "  with open(os.path.join(save_path, 'scores_large_' + str(i) + '.pkl'), 'rb') as f:\n",
        "    scores = pickle.load(f)\n",
        "\n",
        "  model.fit(epochs = 20, shuffle = True, x = inputs, y = np.array(scores), batch_size = 256, validation_split = 0.01)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
